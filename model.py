import pandas as pd
import numpy as np
import os
import glob
import requests
import json
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import warnings
from tqdm import tqdm

# ===================== 0. é…ç½®åŒºåŸŸ =====================
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# è·¯å¾„é…ç½®
DATA_PATH = r"D:\LSTM\data"
OUTPUT_DIR = r"D:\LSTM\model_benchmark_v17_final"
LSTM_MODEL_PATH = r"D:\LSTM\zeroday_lstm.h5"
OLLAMA_API = "http://localhost:11434/api/generate"

# å¾…æµ‹è¯•çš„ LLM æ¨¡å‹
LLM_CANDIDATES = [
    "llama3_LSTM:latest",
    "DeepSeek-R1_LSTM:latest",
    "gemma2_LSTM:latest",
    "qwen3_LSTM:latest"
]

# æ ·æœ¬è®¾ç½®ï¼š50æ”»å‡» + 50æ­£å¸¸ = 100æ¡
N_SAMPLES = 50

# æ··åˆæ¨¡å‹ç½®ä¿¡åº¦è§¦å‘åŒºé—´ (0.3 ~ 0.7 ä¹‹é—´äº¤ç»™ LLM)
CONF_LOW = 0.3
CONF_HIGH = 0.7

if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('ggplot')

# ===================== 1. æ ¸å¿ƒå·¥å…·å‡½æ•° =====================

# --- åŠ è½½ LSTM æ¨¡å‹ ---
print(f"ğŸ”§ [Init] Loading LSTM Model...")
lstm_model = None
try:
    # å°è¯•æ ‡å‡†åŠ è½½
    lstm_model = tf.keras.models.load_model(LSTM_MODEL_PATH)
    print("âœ… LSTM Model Loaded Successfully!")
except Exception:
    print(f"âš ï¸ Standard load failed, trying compile=False...")
    try:
        # å…¼å®¹æ€§åŠ è½½æ¨¡å¼
        lstm_model = tf.keras.models.load_model(LSTM_MODEL_PATH, compile=False)
        print("âœ… LSTM Model Loaded (No Compile Mode)!")
    except Exception as e:
        print(f"âŒ LSTM Load FAILED. Predictions will be 0.")
        print(f"Error: {e}")

# --- æ•°æ®é¢„å¤„ç† (å½’ä¸€åŒ–) ---
scaler = MinMaxScaler()
IS_SCALER_FITTED = False


def preprocess_for_lstm_optimized(df_features):
    global IS_SCALER_FITTED, scaler
    data = df_features.replace([np.inf, -np.inf], 0).fillna(0)

    # æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„æ‹Ÿåˆ
    if not IS_SCALER_FITTED:
        scaler.fit(data)
        IS_SCALER_FITTED = True

    scaled_data = scaler.transform(data)
    # Reshape to (Batch, TimeSteps, Features)
    return np.expand_dims(scaled_data, axis=1)


# --- LLM æç¤ºè¯ç”Ÿæˆ ---
def get_smart_prompt(model_name, features):
    data_json = json.dumps(features, indent=2)

    # DeepSeek ä¸“ç”¨æç¤ºè¯ (ä¾§é‡æ¨ç†)
    if "deepseek" in model_name.lower():
        return f"""
        [Role]: Cybersecurity Forensic Analyst.
        [Input Packet]: {data_json}
        [Task]: Determine if this is a 'Web Attack' or 'Benign'.
        [Logic]:
        - Check 'Avg Size'. Web attacks (SQLi/XSS) often have irregular lengths.
        - Check 'Duration'. Port scans are short; DoS is long.
        [Output]: If suspicious, output "ATTACK". If normal, output "BENIGN".
        """

    # Llama3/Qwen/Gemma é€šç”¨æç¤ºè¯
    return f"""
    Analyze the following network traffic features: {data_json}
    Classify as "ATTACK" or "BENIGN".
    Rules:
    1. Unexpected high ports + short duration -> Suspicious.
    2. Large total bytes but few packets -> Suspicious (Data Exfiltration).
    3. Standard web ports (80/443) -> Likely Benign unless pattern matches injection.

    Respond JSON: {{ "prediction": "ATTACK" or "BENIGN" }}
    """


# --- æŸ¥è¯¢ LLM ---
def query_llm(model_name, features):
    try:
        response = requests.post(OLLAMA_API, json={
            "model": model_name,
            "prompt": get_smart_prompt(model_name, features),
            "stream": False,
            "format": "json" if "llama" not in model_name else ""
        }, timeout=20)  # 20ç§’è¶…æ—¶

        if response.status_code == 200:
            txt = response.json()['response'].upper()
            if "ATTACK" in txt: return "ATTACK"
            if "BENIGN" in txt: return "BENIGN"
    except:
        pass
    return "BENIGN"  # å…œåº•ç­–ç•¥ï¼šæŠ¥é”™å½“æˆæ­£å¸¸


# --- åŠ è½½æ•°æ® ---
def load_and_prep_data():
    all_files = glob.glob(os.path.join(DATA_PATH, "*.csv"))
    attack_list, benign_list = [], []

    print("ğŸ“‚ [Init] Loading CSV Data...")
    for f in all_files:
        # ç®€å•è¿‡æ»¤æ–‡ä»¶å
        if "WebAttacks" not in f and "Thursday" not in f: continue
        try:
            df = pd.read_csv(f, encoding='cp1252')
            df.columns = [c.strip() for c in df.columns]

            # æ ‡ç­¾è¯†åˆ«
            mask_attack = df['Label'].astype(str).apply(lambda x: any(k in x for k in ['Web', 'Sql', 'XSS', 'Brute']))
            attacks = df[mask_attack].copy()
            benigns = df[df['Label'] == 'BENIGN'].copy()

            if not attacks.empty: attack_list.append(attacks)
            if not benigns.empty: benign_list.append(benigns)
        except:
            continue

    if not attack_list or not benign_list:
        print("âŒ No valid data found!")
        return None, None, None

    # Isolation Forest è®­ç»ƒ (ç”¨å‰1000æ¡æ­£å¸¸æ•°æ®)
    train_benign = pd.concat(benign_list).head(1000).select_dtypes(include=[np.number]).fillna(0)
    iso_forest = IsolationForest(n_estimators=1000, contamination=0.1, random_state=42)
    iso_forest.fit(train_benign)
    feature_cols = train_benign.columns.tolist()

    # é‡‡æ · (50 + 50)
    n_attack = min(len(pd.concat(attack_list)), N_SAMPLES)
    n_benign = min(len(pd.concat(benign_list)), N_SAMPLES)

    test_df = pd.concat([
        pd.concat(attack_list).sample(n=N_SAMPLES, replace=(n_attack < N_SAMPLES), random_state=42),
        pd.concat(benign_list).sample(n=N_SAMPLES, replace=(n_benign < N_SAMPLES), random_state=42)
    ]).sample(frac=1).reset_index(drop=True)

    print(f"ğŸ“Š Dataset Ready: {len(test_df)} samples (Balanced)")
    return test_df, iso_forest, feature_cols


# ===================== 2. ç»˜å›¾å‡½æ•° =====================

def plot_single_model(model_name, metrics):
    """ä¸ºæ¯ä¸ªæ¨¡å‹ç”»ä¸€å¼ å•ç‹¬çš„å›¾"""
    labels = list(metrics.keys())
    values = list(metrics.values())

    plt.figure(figsize=(8, 6))
    bars = plt.bar(labels, values, color=['#4c72b0', '#55a868', '#c44e52', '#8172b3'])

    plt.ylim(0, 1.15)
    plt.title(f"Performance: {model_name}\n(N={N_SAMPLES * 2})")
    plt.ylabel("Score")

    # æ ‡æ•°å€¼
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2., height,
                 f'{height:.2f}', ha='center', va='bottom', fontweight='bold')

    safe_name = model_name.split(":")[0]
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, f"1_Individual_{safe_name}.png"))
    plt.close()


def plot_summary(all_results):
    """æ±‡æ€»å¯¹æ¯”å›¾"""
    df = pd.DataFrame(all_results).set_index("Model")

    # ä½¿ç”¨ seaborn è°ƒè‰²æ¿
    ax = df.plot(kind='bar', figsize=(14, 7), width=0.85, edgecolor='black', alpha=0.9)

    plt.title(f"Benchmark Summary: Hybrid LSTM-LLM vs Baseline (N={N_SAMPLES * 2})", fontsize=16)
    plt.ylabel("Score", fontsize=14)
    plt.ylim(0, 1.2)
    plt.xticks(rotation=0, fontsize=11)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4, fontsize=11)
    plt.grid(axis='y', linestyle='--', alpha=0.5)

    # åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ ‡æ•°å€¼
    for container in ax.containers:
        ax.bar_label(container, fmt='%.2f', padding=3, fontsize=9, rotation=90)

    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "2_Summary_Comparison.png"), dpi=300)
    print(f"\nâœ… Summary Plot Saved to: {os.path.join(OUTPUT_DIR, '2_Summary_Comparison.png')}")
    plt.close()


# ===================== 3. ä¸»ç¨‹åº =====================

def main():
    df, iso_model, feature_cols = load_and_prep_data()
    if df is None: return

    # --- 1. åŸºç¡€æ¨¡å‹é¢„æµ‹ (LSTM + IsoForest) ---
    print("âš¡ Calculating Base Metrics (LSTM & IsoForest)...")
    feat_data = df[feature_cols]

    # LSTM é¢„æµ‹ (å¸¦ä¿®å¤é€»è¾‘)
    if lstm_model:
        lstm_inputs = preprocess_for_lstm_optimized(feat_data)
        raw_preds = lstm_model.predict(lstm_inputs, verbose=0)

        # [æ ¸å¿ƒä¿®å¤] åˆ¤æ–­è¾“å‡ºç»´åº¦
        if raw_preds.shape[-1] == 2:
            # å¦‚æœè¾“å‡ºæ˜¯ [Neg_Prob, Pos_Prob]ï¼Œå–ç¬¬äºŒåˆ—
            lstm_probs = raw_preds[:, 1]
        else:
            # å¦‚æœè¾“å‡ºæ˜¯ [Pos_Prob]ï¼Œç›´æ¥å±•å¹³
            lstm_probs = raw_preds.flatten()

        print(f"   ğŸ” LSTM Output Processed. Shape: {lstm_probs.shape}")
    else:
        lstm_probs = np.zeros(len(df))  # å…¨0

    iso_preds = iso_model.predict(feat_data.fillna(0))

    # --- 2. å‡†å¤‡ LSTM åŸºå‡†çº¿æ•°æ® ---
    # çœŸå®æ ‡ç­¾è½¬æ¢ (1=Attack, 0=Benign)
    y_true_global = [1 if any(k in str(label) for k in ['Web', 'Sql', 'XSS', 'Brute']) else 0 for label in df['Label']]
    # LSTM é¢„æµ‹è½¬æ¢
    lstm_preds_binary = [1 if p > 0.5 else 0 for p in lstm_probs]

    summary_metrics = []

    # è®¡ç®— LSTM åŸºå‡†æŒ‡æ ‡
    lstm_metrics = {
        "Accuracy": accuracy_score(y_true_global, lstm_preds_binary),
        "Recall": recall_score(y_true_global, lstm_preds_binary),
        "Precision": precision_score(y_true_global, lstm_preds_binary, zero_division=0),
        "F1": f1_score(y_true_global, lstm_preds_binary)
    }

    summary_entry = lstm_metrics.copy()
    summary_entry["Model"] = "LSTM (Baseline)"
    summary_metrics.append(summary_entry)

    # ç”» LSTM å›¾
    plot_single_model("LSTM_Only", lstm_metrics)
    print(f"   ğŸ“Š LSTM Baseline F1: {lstm_metrics['F1']:.3f}")

    # --- 3. éå† LLM æ··åˆæ¨¡å‹ ---
    print("\nğŸš€ Starting Hybrid LLM Testing...")

    # é¢„å…ˆè®¡ç®—è§¦å‘æ¡ä»¶ï¼Œå‡å°‘å¾ªç¯å†…çš„è®¡ç®—
    base_records = []
    for i in range(len(df)):
        p = lstm_probs[i]
        iso = iso_preds[i]
        # è§¦å‘é€»è¾‘: LSTM ä¸ç¡®å®š (0.3~0.7) æˆ– LSTM æ¼æŠ¥ä½† IsoForest æŠ¥è­¦
        trigger = (CONF_LOW < p < CONF_HIGH) or (p < CONF_LOW and iso == -1)
        base_records.append({"prob": p, "trigger": trigger, "row": df.iloc[i]})

    for model_name in LLM_CANDIDATES:
        safe_name = model_name.split(":")[0]
        print(f"ğŸ‘‰ Testing: {safe_name}")

        y_pred_hybrid = []
        llm_call_count = 0

        # è¿›åº¦æ¡
        for rec in tqdm(base_records, leave=False, desc=safe_name):
            if rec['trigger']:
                llm_call_count += 1
                # å‡†å¤‡æœ€å…³é”®çš„ç‰¹å¾å‘ç»™ LLM
                row = rec['row']
                feat = {
                    "Dst Port": int(row.get("Destination Port", 0)),
                    "Duration": float(row.get("Flow Duration", 0)),
                    "Avg Size": float(row.get("Total Length of Fwd Packets", 0)) / max(1, float(
                        row.get("Total Fwd Packets", 0)))
                }
                pred_label = query_llm(model_name, feat)
                y_pred_hybrid.append(1 if pred_label == "ATTACK" else 0)
            else:
                # æ²¡è§¦å‘ -> å¬ LSTM çš„
                y_pred_hybrid.append(1 if rec['prob'] > 0.5 else 0)

        # è®¡ç®—æŒ‡æ ‡
        curr_metrics = {
            "Accuracy": accuracy_score(y_true_global, y_pred_hybrid),
            "Recall": recall_score(y_true_global, y_pred_hybrid),
            "Precision": precision_score(y_true_global, y_pred_hybrid, zero_division=0),
            "F1": f1_score(y_true_global, y_pred_hybrid)
        }

        print(f"   ğŸ“ LLM Calls: {llm_call_count} | F1: {curr_metrics['F1']:.3f}")

        # ç»˜å›¾ & è®°å½•
        plot_single_model(safe_name, curr_metrics)

        summary_entry = curr_metrics.copy()
        summary_entry["Model"] = safe_name
        summary_metrics.append(summary_entry)

    # --- 4. æœ€ç»ˆæ±‡æ€» ---
    print("\nğŸ¨ Generating Summary Chart...")
    plot_summary(summary_metrics)
    print("\nğŸ‰ All Done! Results are in:", OUTPUT_DIR)


if __name__ == "__main__":
    main()